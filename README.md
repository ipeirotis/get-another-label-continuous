## Description

Crowdsourcing techniques in platforms like Mechanical Turk and other "micro-outsourcing" platforms allow the easy collection of annotation data from a wide variety of online workers. The results from the workers are of imperfect quality: 
• No worker is perfect and each worker commits different types of errors. 
• Some workers are nothing more than spammers doing minimal, if any, real work.

Most of the quality control techniques are designed to operate with discrete answers. In this algorithm, we create a tool that, in the presence of noisy workers, and answers that are a
 continuous variable
• Estimates the most likely answer values
• Estimates the quality of the workers that return the answers

The idea behind this work is an extension in continuous data of what is described in Quality Management on Amazon Mechanical Turk, by Ipeirotis, Wang, and Provost, which was presented at HCOMP 2010.

## How to Run

For details on how to run the algorithm see https://github.com/ipeirotis/get-another-label-continuous/wiki/How-to-Run-Get-Another-Label-Continuous 


